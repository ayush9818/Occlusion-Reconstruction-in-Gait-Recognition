{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer=SummaryWriter('/user1/faculty/cvpr/ujjwal/dhritimaan/GANs/datasets/log_cycle_gan/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceFolder(data.Dataset):\n",
    "    def __init__(self, trainA,trainB):\n",
    "        self.trainA=trainA\n",
    "        self.trainB=trainB\n",
    "\n",
    "    def __len__(self):\n",
    "        return 400\n",
    "    def __getitem__(self, index):\n",
    "        imgA=plt.imread(self.trainA[index])\n",
    "        imgB=plt.imread(self.trainB[index])\n",
    "        dict_img={'A':imgA,'B':imgB}\n",
    "        return dict_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "a=os.listdir('/user1/faculty/cvpr/ujjwal/dhritimaan/GANs/datasets/facades/trainA')\n",
    "b=os.listdir('/user1/faculty/cvpr/ujjwal/dhritimaan/GANs/datasets/facades/trainB')\n",
    "lista=[]\n",
    "listb=[]\n",
    "for i in range(400):\n",
    "    lista.append(os.path.join('/user1/faculty/cvpr/ujjwal/dhritimaan/GANs/datasets/facades/trainA',a[i]))\n",
    "    listb.append(os.path.join('/user1/faculty/cvpr/ujjwal/dhritimaan/GANs/datasets/facades/trainB',b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=SequenceFolder(lista,listb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=torch.utils.data.DataLoader(train,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for a,b in enumerate(dataloader):\n",
    "    a\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['A'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['B'].permute(0,3,1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "        Parameters:\n",
    "            input_nc (int)      -- the number of channels in input images\n",
    "            output_nc (int)     -- the number of channels in output images\n",
    "            ngf (int)           -- the number of filters in the last conv layer\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=torch.device('cuda:1')\n",
    "genAB=ResnetGenerator(3,3).to(cuda)\n",
    "genBA=ResnetGenerator(3,3).to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "discAB=NLayerDiscriminator(3,3).to(cuda)\n",
    "discBA=NLayerDiscriminator(3,3).to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optim=optim.Adam(chain(genAB.parameters(),genBA.parameters()),lr=0.0002,betas=(0.5,0.999))\n",
    "dis_optim=optim.Adam(chain(discAB.parameters(),discBA.parameters()),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_ce=nn.NLLLoss()\n",
    "#loss_mse=nn.MSELoss()\n",
    "#loss_rec=l1loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1loss(x,y):\n",
    "    return torch.mean(torch.abs(x-y))\n",
    "def loss_mse(x,y):\n",
    "    return torch.mean((x-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  disc_loss:0.657579243183136  gen_loss:24.917217254638672 recA Loss:0.8229697346687317 recB Loss:0.5846944451332092\n",
      "epoch:0  disc_loss:0.587417483329773  gen_loss:17.47452163696289 recA Loss:0.8506605625152588 recB Loss:0.4391506314277649\n",
      "epoch:0  disc_loss:0.5253570079803467  gen_loss:13.311656951904297 recA Loss:0.891198456287384 recB Loss:0.35343021154403687\n",
      "epoch:0  disc_loss:0.42379483580589294  gen_loss:11.720146179199219 recA Loss:0.8868804574012756 recB Loss:0.2952173948287964\n",
      "epoch:0  disc_loss:0.37030428647994995  gen_loss:10.022720336914062 recA Loss:0.9036351442337036 recB Loss:0.250381201505661\n",
      "epoch:0  disc_loss:0.3405129909515381  gen_loss:9.80846881866455 recA Loss:0.9597069025039673 recB Loss:0.23879751563072205\n",
      "epoch:0  disc_loss:0.3320743441581726  gen_loss:10.110002517700195 recA Loss:0.9626301527023315 recB Loss:0.2594616711139679\n",
      "epoch:0  disc_loss:0.30445924401283264  gen_loss:8.958944320678711 recA Loss:1.003706932067871 recB Loss:0.1974547952413559\n",
      "epoch:0  disc_loss:0.29401645064353943  gen_loss:8.502704620361328 recA Loss:0.9829961061477661 recB Loss:0.18259936571121216\n",
      "epoch:0  disc_loss:0.2748223841190338  gen_loss:7.794007301330566 recA Loss:1.0284008979797363 recB Loss:0.1729087084531784\n",
      "epoch:0  disc_loss:0.24658745527267456  gen_loss:7.614221572875977 recA Loss:1.017755150794983 recB Loss:0.14557254314422607\n",
      "epoch:0  disc_loss:0.2328135371208191  gen_loss:9.1978759765625 recA Loss:1.0322679281234741 recB Loss:0.1648038923740387\n",
      "epoch:0  disc_loss:0.24100419878959656  gen_loss:8.519437789916992 recA Loss:1.0495996475219727 recB Loss:0.15202902257442474\n",
      "epoch:0  disc_loss:0.25030335783958435  gen_loss:9.04899787902832 recA Loss:0.9875499606132507 recB Loss:0.18260709941387177\n",
      "epoch:0  disc_loss:0.2102876901626587  gen_loss:8.204314231872559 recA Loss:1.038069725036621 recB Loss:0.16519221663475037\n",
      "epoch:0  disc_loss:0.1923270970582962  gen_loss:7.869051456451416 recA Loss:1.0400601625442505 recB Loss:0.16379335522651672\n",
      "epoch:0  disc_loss:0.16961033642292023  gen_loss:7.168168544769287 recA Loss:1.046384334564209 recB Loss:0.14072558283805847\n",
      "epoch:0  disc_loss:0.18127501010894775  gen_loss:7.222545623779297 recA Loss:1.044818639755249 recB Loss:0.1317165493965149\n",
      "epoch:0  disc_loss:0.15482667088508606  gen_loss:6.930854797363281 recA Loss:1.0448861122131348 recB Loss:0.11379405111074448\n",
      "epoch:0  disc_loss:0.14713509380817413  gen_loss:7.346218109130859 recA Loss:1.034796118736267 recB Loss:0.11557602882385254\n",
      "epoch:0  disc_loss:0.19106708467006683  gen_loss:6.867960453033447 recA Loss:1.044567346572876 recB Loss:0.11455514281988144\n",
      "epoch:0  disc_loss:0.1406249850988388  gen_loss:6.3363037109375 recA Loss:1.041262149810791 recB Loss:0.0997394546866417\n",
      "epoch:0  disc_loss:0.13413846492767334  gen_loss:7.304680347442627 recA Loss:1.0411899089813232 recB Loss:0.12617990374565125\n",
      "epoch:0  disc_loss:0.12171366065740585  gen_loss:6.407339096069336 recA Loss:1.0483717918395996 recB Loss:0.09720810502767563\n",
      "epoch:0  disc_loss:0.13349856436252594  gen_loss:7.076597690582275 recA Loss:1.0561076402664185 recB Loss:0.1530286967754364\n",
      "epoch:0  disc_loss:0.12397128343582153  gen_loss:7.124015808105469 recA Loss:1.0535650253295898 recB Loss:0.10299582779407501\n",
      "epoch:0  disc_loss:0.1295887529850006  gen_loss:6.278079032897949 recA Loss:1.0359891653060913 recB Loss:0.11838366836309433\n",
      "epoch:0  disc_loss:0.12461832165718079  gen_loss:6.001170635223389 recA Loss:1.0596998929977417 recB Loss:0.10340391099452972\n",
      "epoch:0  disc_loss:0.11958324909210205  gen_loss:6.5207295417785645 recA Loss:1.0579701662063599 recB Loss:0.10076940059661865\n",
      "epoch:0  disc_loss:0.10562965273857117  gen_loss:5.900524139404297 recA Loss:1.0490128993988037 recB Loss:0.10477800667285919\n",
      "epoch:0  disc_loss:0.10604262351989746  gen_loss:6.384973526000977 recA Loss:1.047872543334961 recB Loss:0.11130596697330475\n",
      "epoch:0  disc_loss:0.09603831171989441  gen_loss:6.073877334594727 recA Loss:1.0725051164627075 recB Loss:0.08452707529067993\n",
      "epoch:0  disc_loss:0.1039561778306961  gen_loss:6.495969295501709 recA Loss:1.0494627952575684 recB Loss:0.0819389596581459\n",
      "epoch:0  disc_loss:0.09056723862886429  gen_loss:5.582917213439941 recA Loss:1.0474741458892822 recB Loss:0.08754923939704895\n",
      "epoch:0  disc_loss:0.09088272601366043  gen_loss:5.71266508102417 recA Loss:1.0679150819778442 recB Loss:0.08210985362529755\n",
      "epoch:0  disc_loss:0.09544092416763306  gen_loss:5.868011474609375 recA Loss:1.0509394407272339 recB Loss:0.10676948726177216\n",
      "epoch:0  disc_loss:0.08385593444108963  gen_loss:6.3404951095581055 recA Loss:1.0647858381271362 recB Loss:0.1004878506064415\n",
      "epoch:0  disc_loss:0.09643475711345673  gen_loss:5.875030040740967 recA Loss:1.0632485151290894 recB Loss:0.08553081005811691\n",
      "epoch:0  disc_loss:0.09326334297657013  gen_loss:5.518191337585449 recA Loss:1.0476752519607544 recB Loss:0.08692200481891632\n",
      "epoch:0  disc_loss:0.08555309474468231  gen_loss:5.960104942321777 recA Loss:1.0721124410629272 recB Loss:0.10094984620809555\n",
      "epoch:0  disc_loss:0.0961126983165741  gen_loss:6.422214984893799 recA Loss:1.0422372817993164 recB Loss:0.09561014175415039\n",
      "epoch:0  disc_loss:0.09041044116020203  gen_loss:6.937246799468994 recA Loss:1.0749452114105225 recB Loss:0.15254954993724823\n",
      "epoch:0  disc_loss:0.07554320991039276  gen_loss:5.790309906005859 recA Loss:1.0448225736618042 recB Loss:0.09399597346782684\n",
      "epoch:0  disc_loss:0.08446113020181656  gen_loss:6.338583946228027 recA Loss:1.0589054822921753 recB Loss:0.11096887290477753\n",
      "epoch:0  disc_loss:0.06980353593826294  gen_loss:5.793810844421387 recA Loss:1.0574921369552612 recB Loss:0.10305561870336533\n",
      "epoch:0  disc_loss:0.06648210436105728  gen_loss:5.458168983459473 recA Loss:1.04156494140625 recB Loss:0.08053569495677948\n",
      "epoch:0  disc_loss:0.07770505547523499  gen_loss:5.9192962646484375 recA Loss:1.0736970901489258 recB Loss:0.08112545311450958\n",
      "epoch:0  disc_loss:0.066867396235466  gen_loss:6.151033401489258 recA Loss:1.056088924407959 recB Loss:0.10282208025455475\n",
      "epoch:0  disc_loss:0.06662268191576004  gen_loss:5.678276538848877 recA Loss:1.0485548973083496 recB Loss:0.10228784382343292\n",
      "epoch:0  disc_loss:0.06737324595451355  gen_loss:5.286967754364014 recA Loss:1.0778701305389404 recB Loss:0.08577272295951843\n",
      "epoch:0  disc_loss:0.06524188816547394  gen_loss:5.316659450531006 recA Loss:1.0357656478881836 recB Loss:0.07360884547233582\n",
      "epoch:0  disc_loss:0.06023334711790085  gen_loss:5.155716896057129 recA Loss:1.062975287437439 recB Loss:0.08114752918481827\n",
      "epoch:0  disc_loss:0.05576309561729431  gen_loss:5.205046653747559 recA Loss:1.0575618743896484 recB Loss:0.07457099854946136\n",
      "epoch:0  disc_loss:0.07218606024980545  gen_loss:5.968088150024414 recA Loss:1.0439794063568115 recB Loss:0.11235411465167999\n",
      "epoch:0  disc_loss:0.06046855449676514  gen_loss:5.88873291015625 recA Loss:1.0635377168655396 recB Loss:0.0999181792140007\n",
      "epoch:0  disc_loss:0.060993850231170654  gen_loss:6.6684675216674805 recA Loss:1.0481172800064087 recB Loss:0.10267772525548935\n",
      "epoch:0  disc_loss:0.06360358744859695  gen_loss:6.101809501647949 recA Loss:1.0483462810516357 recB Loss:0.08290114253759384\n",
      "epoch:0  disc_loss:0.055065661668777466  gen_loss:6.308398246765137 recA Loss:1.0473518371582031 recB Loss:0.12418776750564575\n",
      "epoch:0  disc_loss:0.055712707340717316  gen_loss:5.29926061630249 recA Loss:1.0546756982803345 recB Loss:0.0853484496474266\n",
      "epoch:0  disc_loss:0.058290086686611176  gen_loss:5.81141471862793 recA Loss:1.0431047677993774 recB Loss:0.10751886665821075\n",
      "epoch:0  disc_loss:0.05745275318622589  gen_loss:5.170222282409668 recA Loss:1.0544254779815674 recB Loss:0.08530014753341675\n",
      "epoch:0  disc_loss:0.056980639696121216  gen_loss:5.054471969604492 recA Loss:1.061977744102478 recB Loss:0.07176901400089264\n",
      "epoch:0  disc_loss:0.051374852657318115  gen_loss:5.758687496185303 recA Loss:1.0362229347229004 recB Loss:0.0922316312789917\n",
      "epoch:0  disc_loss:0.05034365877509117  gen_loss:5.635466575622559 recA Loss:1.067070484161377 recB Loss:0.06783364713191986\n",
      "epoch:0  disc_loss:0.05197947844862938  gen_loss:5.513114929199219 recA Loss:1.0373382568359375 recB Loss:0.08584390580654144\n",
      "epoch:0  disc_loss:0.04602377861738205  gen_loss:5.3379011154174805 recA Loss:1.050654649734497 recB Loss:0.08347052335739136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  disc_loss:0.05575686693191528  gen_loss:5.542117118835449 recA Loss:1.0588988065719604 recB Loss:0.09231182187795639\n",
      "epoch:0  disc_loss:0.04590648040175438  gen_loss:5.826559066772461 recA Loss:1.0346274375915527 recB Loss:0.09491991251707077\n",
      "epoch:0  disc_loss:0.05688507854938507  gen_loss:5.96302604675293 recA Loss:1.0490026473999023 recB Loss:0.1127542182803154\n",
      "epoch:0  disc_loss:0.04481648653745651  gen_loss:5.899209976196289 recA Loss:1.0586169958114624 recB Loss:0.09704550355672836\n",
      "epoch:0  disc_loss:0.036580491811037064  gen_loss:5.280946731567383 recA Loss:1.0578445196151733 recB Loss:0.08997762203216553\n",
      "epoch:0  disc_loss:0.04995495080947876  gen_loss:5.420129299163818 recA Loss:1.0437637567520142 recB Loss:0.07822389155626297\n",
      "epoch:0  disc_loss:0.04017842188477516  gen_loss:5.800753593444824 recA Loss:1.0438357591629028 recB Loss:0.08848973363637924\n",
      "epoch:0  disc_loss:0.03752864897251129  gen_loss:5.112937927246094 recA Loss:1.0494065284729004 recB Loss:0.06589595228433609\n",
      "epoch:0  disc_loss:0.05805416405200958  gen_loss:4.953269004821777 recA Loss:1.0580230951309204 recB Loss:0.0677088126540184\n",
      "epoch:0  disc_loss:0.045528657734394073  gen_loss:6.346973419189453 recA Loss:1.0587137937545776 recB Loss:0.12558703124523163\n",
      "epoch:0  disc_loss:0.04344400390982628  gen_loss:6.106573104858398 recA Loss:1.0166537761688232 recB Loss:0.09937363862991333\n",
      "epoch:0  disc_loss:0.039240866899490356  gen_loss:5.726269721984863 recA Loss:1.0253783464431763 recB Loss:0.09392291307449341\n",
      "epoch:0  disc_loss:0.038650114089250565  gen_loss:5.4085588455200195 recA Loss:1.0326462984085083 recB Loss:0.08566808700561523\n",
      "epoch:0  disc_loss:0.03340372443199158  gen_loss:5.776755332946777 recA Loss:1.0301250219345093 recB Loss:0.09764300286769867\n",
      "epoch:0  disc_loss:0.034169264137744904  gen_loss:5.764557838439941 recA Loss:1.052120566368103 recB Loss:0.07220235466957092\n",
      "epoch:0  disc_loss:0.03582921251654625  gen_loss:5.391596794128418 recA Loss:1.048098087310791 recB Loss:0.07895240932703018\n",
      "epoch:0  disc_loss:0.03668365254998207  gen_loss:5.384938716888428 recA Loss:1.0341200828552246 recB Loss:0.08634885400533676\n",
      "epoch:0  disc_loss:0.04059317335486412  gen_loss:5.1469621658325195 recA Loss:1.0446734428405762 recB Loss:0.07419153302907944\n",
      "epoch:0  disc_loss:0.029564719647169113  gen_loss:6.003931522369385 recA Loss:1.0288851261138916 recB Loss:0.09583568572998047\n",
      "epoch:0  disc_loss:0.032668761909008026  gen_loss:6.036863327026367 recA Loss:1.028319001197815 recB Loss:0.08807677030563354\n",
      "epoch:0  disc_loss:0.0307331420481205  gen_loss:6.247092247009277 recA Loss:1.0356481075286865 recB Loss:0.11965879797935486\n",
      "epoch:0  disc_loss:0.03261012211441994  gen_loss:5.53214168548584 recA Loss:1.0250638723373413 recB Loss:0.08907242119312286\n",
      "epoch:0  disc_loss:0.0278526209294796  gen_loss:5.290347099304199 recA Loss:1.0358035564422607 recB Loss:0.07697775959968567\n",
      "epoch:0  disc_loss:0.031680069863796234  gen_loss:4.985836505889893 recA Loss:1.0401970148086548 recB Loss:0.08072847872972488\n",
      "epoch:0  disc_loss:0.03465873748064041  gen_loss:4.979250907897949 recA Loss:1.0359699726104736 recB Loss:0.0786873921751976\n",
      "epoch:0  disc_loss:0.039465710520744324  gen_loss:4.9404497146606445 recA Loss:1.02370023727417 recB Loss:0.08212535083293915\n",
      "epoch:0  disc_loss:0.02824445068836212  gen_loss:5.336799621582031 recA Loss:1.0310709476470947 recB Loss:0.07549761235713959\n",
      "epoch:0  disc_loss:0.03476251661777496  gen_loss:5.712425231933594 recA Loss:1.0468653440475464 recB Loss:0.08346748352050781\n",
      "epoch:0  disc_loss:0.035709451884031296  gen_loss:5.40162992477417 recA Loss:1.0449658632278442 recB Loss:0.08433496206998825\n",
      "epoch:0  disc_loss:0.051565077155828476  gen_loss:6.597667694091797 recA Loss:1.0374547243118286 recB Loss:0.10083618760108948\n",
      "epoch:0  disc_loss:0.03857731819152832  gen_loss:6.062577247619629 recA Loss:1.0127573013305664 recB Loss:0.10228552669286728\n",
      "epoch:0  disc_loss:0.02972593903541565  gen_loss:5.807278633117676 recA Loss:1.0292575359344482 recB Loss:0.10425429046154022\n",
      "epoch:0  disc_loss:0.03464522212743759  gen_loss:8.071137428283691 recA Loss:1.0331662893295288 recB Loss:0.10264988988637924\n",
      "epoch:0  disc_loss:0.02872718684375286  gen_loss:7.628084659576416 recA Loss:1.036505937576294 recB Loss:0.08616364002227783\n",
      "epoch:1  disc_loss:0.04168283939361572  gen_loss:5.467516899108887 recA Loss:1.0227677822113037 recB Loss:0.09567990899085999\n",
      "epoch:1  disc_loss:0.0376008041203022  gen_loss:6.08395528793335 recA Loss:1.032725214958191 recB Loss:0.09454705566167831\n",
      "epoch:1  disc_loss:0.025992950424551964  gen_loss:5.495428085327148 recA Loss:1.0169254541397095 recB Loss:0.08259295672178268\n",
      "epoch:1  disc_loss:0.030671576038002968  gen_loss:5.998933792114258 recA Loss:1.042828917503357 recB Loss:0.09967364370822906\n",
      "epoch:1  disc_loss:0.039563003927469254  gen_loss:5.610322952270508 recA Loss:1.0453426837921143 recB Loss:0.07812443375587463\n",
      "epoch:1  disc_loss:0.024713627994060516  gen_loss:5.163092136383057 recA Loss:1.0233980417251587 recB Loss:0.06433951109647751\n",
      "epoch:1  disc_loss:0.028938213363289833  gen_loss:4.531064987182617 recA Loss:1.0261938571929932 recB Loss:0.06350221484899521\n",
      "epoch:1  disc_loss:0.02014240249991417  gen_loss:5.005529403686523 recA Loss:1.0338822603225708 recB Loss:0.05662504583597183\n",
      "epoch:1  disc_loss:0.02817358821630478  gen_loss:5.588473320007324 recA Loss:1.0384480953216553 recB Loss:0.09905444830656052\n",
      "epoch:1  disc_loss:0.034144796431064606  gen_loss:5.255472660064697 recA Loss:1.0301235914230347 recB Loss:0.09013406932353973\n",
      "epoch:1  disc_loss:0.02339029498398304  gen_loss:4.816028118133545 recA Loss:1.0189207792282104 recB Loss:0.07063260674476624\n",
      "epoch:1  disc_loss:0.02549259178340435  gen_loss:4.805876731872559 recA Loss:1.0255320072174072 recB Loss:0.05899570509791374\n",
      "epoch:1  disc_loss:0.018938787281513214  gen_loss:5.076295852661133 recA Loss:1.017655372619629 recB Loss:0.0601830780506134\n",
      "epoch:1  disc_loss:0.023160919547080994  gen_loss:5.212803363800049 recA Loss:1.0286107063293457 recB Loss:0.06212928891181946\n",
      "epoch:1  disc_loss:0.025066640228033066  gen_loss:6.620547294616699 recA Loss:1.0322446823120117 recB Loss:0.11918407678604126\n",
      "epoch:1  disc_loss:0.020209966227412224  gen_loss:4.917582988739014 recA Loss:1.0240960121154785 recB Loss:0.07075384259223938\n",
      "epoch:1  disc_loss:0.02158292755484581  gen_loss:5.415050506591797 recA Loss:1.0220986604690552 recB Loss:0.09970496594905853\n",
      "epoch:1  disc_loss:0.03567667677998543  gen_loss:4.605354309082031 recA Loss:1.0424708127975464 recB Loss:0.0661805272102356\n",
      "epoch:1  disc_loss:0.0225273035466671  gen_loss:5.629894733428955 recA Loss:1.0173227787017822 recB Loss:0.0754866898059845\n",
      "epoch:1  disc_loss:0.02181403525173664  gen_loss:4.806328296661377 recA Loss:1.031286358833313 recB Loss:0.05765063315629959\n",
      "epoch:1  disc_loss:0.0242984127253294  gen_loss:5.537866592407227 recA Loss:1.0301012992858887 recB Loss:0.08903144299983978\n",
      "epoch:1  disc_loss:0.022637708112597466  gen_loss:5.316880226135254 recA Loss:1.0159486532211304 recB Loss:0.09557788074016571\n",
      "epoch:1  disc_loss:0.026862334460020065  gen_loss:5.3429436683654785 recA Loss:1.0290298461914062 recB Loss:0.07424148917198181\n",
      "epoch:1  disc_loss:0.025896139442920685  gen_loss:5.4387359619140625 recA Loss:1.0251648426055908 recB Loss:0.08879046887159348\n",
      "epoch:1  disc_loss:0.018995482474565506  gen_loss:6.169560432434082 recA Loss:1.0148921012878418 recB Loss:0.11011403799057007\n",
      "epoch:1  disc_loss:0.02616196498274803  gen_loss:5.523811340332031 recA Loss:1.0219155550003052 recB Loss:0.09284742176532745\n",
      "epoch:1  disc_loss:0.02697298303246498  gen_loss:4.793087482452393 recA Loss:1.0221151113510132 recB Loss:0.05555585026741028\n",
      "epoch:1  disc_loss:0.022879524156451225  gen_loss:5.5202956199646 recA Loss:1.0433028936386108 recB Loss:0.07386421412229538\n",
      "epoch:1  disc_loss:0.017428619787096977  gen_loss:4.413858413696289 recA Loss:1.0476652383804321 recB Loss:0.05033677816390991\n",
      "epoch:1  disc_loss:0.023096298798918724  gen_loss:4.920430660247803 recA Loss:1.0268354415893555 recB Loss:0.06473217904567719\n",
      "epoch:1  disc_loss:0.015885770320892334  gen_loss:5.770458221435547 recA Loss:1.0501980781555176 recB Loss:0.0694011002779007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1  disc_loss:0.018598351627588272  gen_loss:5.044215202331543 recA Loss:1.0291845798492432 recB Loss:0.05658542737364769\n",
      "epoch:1  disc_loss:0.06438451260328293  gen_loss:5.096673965454102 recA Loss:0.9986088275909424 recB Loss:0.08230998367071152\n",
      "epoch:1  disc_loss:0.024456745013594627  gen_loss:5.68140983581543 recA Loss:1.0266366004943848 recB Loss:0.08128055185079575\n",
      "epoch:1  disc_loss:0.02297811768949032  gen_loss:5.094234466552734 recA Loss:1.0214917659759521 recB Loss:0.08140739053487778\n",
      "epoch:1  disc_loss:0.018566919490695  gen_loss:5.073697090148926 recA Loss:1.0170189142227173 recB Loss:0.06445210427045822\n",
      "epoch:1  disc_loss:0.03241299092769623  gen_loss:4.546093463897705 recA Loss:1.0295071601867676 recB Loss:0.062337130308151245\n",
      "epoch:1  disc_loss:0.05032477155327797  gen_loss:5.649947166442871 recA Loss:1.0408493280410767 recB Loss:0.06253308802843094\n",
      "epoch:1  disc_loss:0.018208706751465797  gen_loss:4.754571914672852 recA Loss:1.0160096883773804 recB Loss:0.07541312277317047\n",
      "epoch:1  disc_loss:0.017359502613544464  gen_loss:5.3592023849487305 recA Loss:1.0284045934677124 recB Loss:0.06493592262268066\n",
      "epoch:1  disc_loss:0.017706766724586487  gen_loss:5.106468677520752 recA Loss:1.0308676958084106 recB Loss:0.06567426025867462\n",
      "epoch:1  disc_loss:0.030635222792625427  gen_loss:5.055839538574219 recA Loss:1.0168875455856323 recB Loss:0.05991149693727493\n",
      "epoch:1  disc_loss:0.020681248977780342  gen_loss:5.239006042480469 recA Loss:1.022875428199768 recB Loss:0.09249164909124374\n",
      "epoch:1  disc_loss:0.0340837724506855  gen_loss:6.074523448944092 recA Loss:1.0259184837341309 recB Loss:0.08665001392364502\n",
      "epoch:1  disc_loss:0.028889697045087814  gen_loss:5.652377128601074 recA Loss:1.0216288566589355 recB Loss:0.11234720796346664\n",
      "epoch:1  disc_loss:0.025750648230314255  gen_loss:6.734679222106934 recA Loss:1.0283160209655762 recB Loss:0.07973893731832504\n",
      "epoch:1  disc_loss:0.026635263115167618  gen_loss:5.88161563873291 recA Loss:1.019981861114502 recB Loss:0.10549718141555786\n",
      "epoch:1  disc_loss:0.02908335067331791  gen_loss:5.399805545806885 recA Loss:1.0451195240020752 recB Loss:0.10420054197311401\n",
      "epoch:1  disc_loss:0.02348276413977146  gen_loss:5.307875633239746 recA Loss:1.0282213687896729 recB Loss:0.08580337464809418\n",
      "epoch:1  disc_loss:0.042046450078487396  gen_loss:4.911255359649658 recA Loss:0.9992915391921997 recB Loss:0.07637515664100647\n",
      "epoch:1  disc_loss:0.03353153169155121  gen_loss:5.036821365356445 recA Loss:1.041819453239441 recB Loss:0.06793241947889328\n",
      "epoch:1  disc_loss:0.02041885070502758  gen_loss:5.300190448760986 recA Loss:1.037876844406128 recB Loss:0.06888456642627716\n",
      "epoch:1  disc_loss:0.0267521683126688  gen_loss:5.026491641998291 recA Loss:1.014592170715332 recB Loss:0.052925556898117065\n",
      "epoch:1  disc_loss:0.02803753688931465  gen_loss:4.529812812805176 recA Loss:1.0205055475234985 recB Loss:0.06027789041399956\n",
      "epoch:1  disc_loss:0.031791672110557556  gen_loss:5.003576755523682 recA Loss:1.0185034275054932 recB Loss:0.06265899538993835\n",
      "epoch:1  disc_loss:0.02132401429116726  gen_loss:4.495541572570801 recA Loss:1.0311026573181152 recB Loss:0.06045961380004883\n",
      "epoch:1  disc_loss:0.026830798014998436  gen_loss:4.456211566925049 recA Loss:1.018685221672058 recB Loss:0.05955731123685837\n",
      "epoch:1  disc_loss:0.02245020493865013  gen_loss:4.91468620300293 recA Loss:1.0080689191818237 recB Loss:0.08059590309858322\n",
      "epoch:1  disc_loss:0.018880967050790787  gen_loss:4.7071075439453125 recA Loss:1.0337767601013184 recB Loss:0.06454406678676605\n",
      "epoch:1  disc_loss:0.013574054464697838  gen_loss:4.562821388244629 recA Loss:1.0287392139434814 recB Loss:0.05545724555850029\n",
      "epoch:1  disc_loss:0.023584716022014618  gen_loss:4.956521034240723 recA Loss:1.0278152227401733 recB Loss:0.0684298425912857\n",
      "epoch:1  disc_loss:0.027428675442934036  gen_loss:4.860363483428955 recA Loss:1.008028507232666 recB Loss:0.057986460626125336\n",
      "epoch:1  disc_loss:0.035607967525720596  gen_loss:7.102422714233398 recA Loss:1.035461664199829 recB Loss:0.08299179375171661\n",
      "epoch:1  disc_loss:0.03261713311076164  gen_loss:6.525345802307129 recA Loss:1.0258156061172485 recB Loss:0.10979430377483368\n",
      "epoch:1  disc_loss:0.026835007593035698  gen_loss:7.357362747192383 recA Loss:1.0199720859527588 recB Loss:0.14664529263973236\n",
      "epoch:1  disc_loss:0.0346762016415596  gen_loss:5.702733039855957 recA Loss:1.0057463645935059 recB Loss:0.11073791980743408\n",
      "epoch:1  disc_loss:0.020268939435482025  gen_loss:5.010792255401611 recA Loss:1.00911545753479 recB Loss:0.07832291722297668\n",
      "epoch:1  disc_loss:0.020360099151730537  gen_loss:5.353702545166016 recA Loss:1.0373132228851318 recB Loss:0.0989350900053978\n",
      "epoch:1  disc_loss:0.02009454369544983  gen_loss:5.088379383087158 recA Loss:1.0135596990585327 recB Loss:0.07301533967256546\n",
      "epoch:1  disc_loss:0.02212780900299549  gen_loss:5.288965225219727 recA Loss:1.0307012796401978 recB Loss:0.08802329003810883\n",
      "epoch:1  disc_loss:0.028265349566936493  gen_loss:4.8340535163879395 recA Loss:1.017785668373108 recB Loss:0.06439805030822754\n",
      "epoch:1  disc_loss:0.03008146584033966  gen_loss:7.4853315353393555 recA Loss:1.00788152217865 recB Loss:0.13248400390148163\n",
      "epoch:1  disc_loss:0.04323318600654602  gen_loss:6.898934364318848 recA Loss:1.0226002931594849 recB Loss:0.13382111489772797\n",
      "epoch:1  disc_loss:0.033743053674697876  gen_loss:7.051422119140625 recA Loss:1.0188608169555664 recB Loss:0.09171265363693237\n",
      "epoch:1  disc_loss:0.024430617690086365  gen_loss:5.585862159729004 recA Loss:1.0091884136199951 recB Loss:0.08856847137212753\n",
      "epoch:1  disc_loss:0.025827419012784958  gen_loss:6.343540191650391 recA Loss:1.0144665241241455 recB Loss:0.07976125180721283\n",
      "epoch:1  disc_loss:0.01818608120083809  gen_loss:5.067893028259277 recA Loss:1.0199304819107056 recB Loss:0.06297629326581955\n",
      "epoch:1  disc_loss:0.020644713193178177  gen_loss:5.2051801681518555 recA Loss:1.020436406135559 recB Loss:0.0751759260892868\n",
      "epoch:1  disc_loss:0.017651058733463287  gen_loss:5.594529151916504 recA Loss:1.0116047859191895 recB Loss:0.0831894725561142\n",
      "epoch:1  disc_loss:0.01924254186451435  gen_loss:4.869729995727539 recA Loss:1.0102550983428955 recB Loss:0.07185835391283035\n",
      "epoch:1  disc_loss:0.01798480935394764  gen_loss:4.7953104972839355 recA Loss:1.0213522911071777 recB Loss:0.07148253172636032\n",
      "epoch:1  disc_loss:0.014084573835134506  gen_loss:5.647733688354492 recA Loss:1.0053935050964355 recB Loss:0.05684460699558258\n",
      "epoch:1  disc_loss:0.01644577831029892  gen_loss:4.783603668212891 recA Loss:1.022584319114685 recB Loss:0.06239243596792221\n",
      "epoch:1  disc_loss:0.01818586140871048  gen_loss:4.853931427001953 recA Loss:1.0411041975021362 recB Loss:0.06766165792942047\n",
      "epoch:1  disc_loss:0.013483474031090736  gen_loss:4.733983039855957 recA Loss:1.0266367197036743 recB Loss:0.0589076392352581\n",
      "epoch:1  disc_loss:0.01937110535800457  gen_loss:4.4478654861450195 recA Loss:1.012306809425354 recB Loss:0.05318383499979973\n",
      "epoch:1  disc_loss:0.014889845624566078  gen_loss:5.273558616638184 recA Loss:1.0134855508804321 recB Loss:0.08276325464248657\n",
      "epoch:1  disc_loss:0.016465354710817337  gen_loss:5.061435699462891 recA Loss:1.0183528661727905 recB Loss:0.07553780823945999\n",
      "epoch:1  disc_loss:0.015491880476474762  gen_loss:4.637904644012451 recA Loss:1.0250725746154785 recB Loss:0.05858403071761131\n",
      "epoch:1  disc_loss:0.01660579815506935  gen_loss:5.2365899085998535 recA Loss:1.01632559299469 recB Loss:0.061624765396118164\n",
      "epoch:1  disc_loss:0.016098998486995697  gen_loss:4.902494430541992 recA Loss:1.0093830823898315 recB Loss:0.05703699588775635\n",
      "epoch:1  disc_loss:0.0167143065482378  gen_loss:4.703506946563721 recA Loss:1.020971655845642 recB Loss:0.06068854033946991\n",
      "epoch:1  disc_loss:0.015693239867687225  gen_loss:4.48740291595459 recA Loss:1.0223848819732666 recB Loss:0.060833051800727844\n",
      "epoch:1  disc_loss:0.014409247785806656  gen_loss:4.5120978355407715 recA Loss:1.0096503496170044 recB Loss:0.05379646271467209\n",
      "epoch:1  disc_loss:0.013743290677666664  gen_loss:4.42145299911499 recA Loss:1.0142964124679565 recB Loss:0.05790526419878006\n",
      "epoch:1  disc_loss:0.01320234127342701  gen_loss:4.675713539123535 recA Loss:1.0003612041473389 recB Loss:0.05772228538990021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1  disc_loss:0.013643580488860607  gen_loss:4.512739181518555 recA Loss:1.0503499507904053 recB Loss:0.05891481041908264\n",
      "epoch:1  disc_loss:0.016882285475730896  gen_loss:4.7121381759643555 recA Loss:1.0291489362716675 recB Loss:0.05358307063579559\n",
      "epoch:1  disc_loss:0.01676405593752861  gen_loss:5.777318000793457 recA Loss:1.0256309509277344 recB Loss:0.09083949029445648\n",
      "epoch:1  disc_loss:0.01353235449641943  gen_loss:4.69516658782959 recA Loss:1.0334796905517578 recB Loss:0.05808669328689575\n",
      "epoch:2  disc_loss:0.013784321956336498  gen_loss:4.6128129959106445 recA Loss:1.0272371768951416 recB Loss:0.0655679702758789\n",
      "epoch:2  disc_loss:0.012395376339554787  gen_loss:4.569788932800293 recA Loss:1.0166682004928589 recB Loss:0.06165506690740585\n",
      "epoch:2  disc_loss:0.012934714555740356  gen_loss:5.0562872886657715 recA Loss:1.0234354734420776 recB Loss:0.07715937495231628\n",
      "epoch:2  disc_loss:0.010235204361379147  gen_loss:4.800635814666748 recA Loss:1.0210286378860474 recB Loss:0.0563991516828537\n",
      "epoch:2  disc_loss:0.010929249227046967  gen_loss:4.56079626083374 recA Loss:1.0263736248016357 recB Loss:0.058625221252441406\n",
      "epoch:2  disc_loss:0.011997835710644722  gen_loss:4.98737907409668 recA Loss:1.0063087940216064 recB Loss:0.06998108327388763\n",
      "epoch:2  disc_loss:0.013093456625938416  gen_loss:5.051895618438721 recA Loss:1.0235364437103271 recB Loss:0.08781413733959198\n",
      "epoch:2  disc_loss:0.016593441367149353  gen_loss:4.600393295288086 recA Loss:1.014259696006775 recB Loss:0.06752313673496246\n",
      "epoch:2  disc_loss:0.014639373868703842  gen_loss:4.952210903167725 recA Loss:1.0389714241027832 recB Loss:0.06992416828870773\n",
      "epoch:2  disc_loss:0.013456033542752266  gen_loss:5.376646995544434 recA Loss:1.0070056915283203 recB Loss:0.07576204091310501\n",
      "epoch:2  disc_loss:0.012351788580417633  gen_loss:5.055811882019043 recA Loss:1.0058122873306274 recB Loss:0.05930514633655548\n",
      "epoch:2  disc_loss:0.014246059581637383  gen_loss:4.475992202758789 recA Loss:1.014542818069458 recB Loss:0.056925974786281586\n",
      "epoch:2  disc_loss:0.01331111416220665  gen_loss:4.5706329345703125 recA Loss:1.0274451971054077 recB Loss:0.05785433202981949\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "count=0\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batches,real_data in enumerate(dataloader):\n",
    "        realA=real_data['A'].permute(0,3,1,2).float().to(cuda)/255\n",
    "        realB=real_data['B'].permute(0,3,1,2).float().to(cuda)/255\n",
    "        \n",
    "        \n",
    "        fakeB=genAB(realA)\n",
    "        fakeA=genBA(realB)\n",
    "        recA=genBA(fakeB)\n",
    "        recB=genAB(fakeA)\n",
    "        dis_real_labB=discAB(realB)\n",
    "        dis_fake_labB=discAB(fakeB)\n",
    "        dis_real_labA=discBA(realA)\n",
    "        dis_fake_labA=discBA(fakeA)\n",
    "        dis_rec_labA=discBA(recA)\n",
    "        dis_rec_labB=discAB(recB)\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        #######training generators\n",
    "        error_genAB=loss_mse(dis_fake_labB,Variable(torch.ones(realA.shape[0],1,30,30).to(cuda)))\n",
    "        error_genBA=loss_mse(dis_fake_labA,Variable(torch.ones(realA.shape[0],1,30,30).to(cuda)))\n",
    "        rec_errorA=l1loss(recA,realA)#loss_rec(recA,realA)\n",
    "        rec_errorB=l1loss(recB,realB)#loss_rec(recB,realB)\n",
    "                            \n",
    "        g_loss=error_genAB+error_genBA+20*rec_errorA+20*rec_errorB\n",
    "        #error_genAB.backward(retain_graph=True)\n",
    "        #error_genBA.backward()\n",
    "        #rec_errorA.backward()\n",
    "        #rec_errorB.backward()\n",
    "        g_loss.backward()#retain_graph=True)\n",
    "        \n",
    "        gen_optim.step()\n",
    "                            \n",
    "                            \n",
    "        #########training discrminators\n",
    "        dis_optim.zero_grad()\n",
    "        \n",
    "        \n",
    "        fakeB=genAB(realA)\n",
    "        fakeA=genBA(realB)\n",
    "        recA=genBA(fakeB)\n",
    "        recB=genAB(fakeA)\n",
    "        dis_real_labB=discAB(realB)\n",
    "        dis_fake_labB=discAB(fakeB)\n",
    "        dis_real_labA=discBA(realA)\n",
    "        dis_fake_labA=discBA(fakeA)\n",
    "        dis_rec_labA=discBA(recA)\n",
    "        dis_rec_labB=discAB(recB)\n",
    "\n",
    "        \n",
    "        \n",
    "        rec_errorA=loss_mse(dis_real_labB,Variable(torch.ones(realA.shape[0],1,30,30).to(cuda)))\n",
    "        error_discAB_with_realB=loss_mse(dis_real_labB,Variable(torch.zeros(realA.shape[0],1,30,30).to(cuda)))\n",
    "        error_discAB_with_fakeB=loss_mse(dis_fake_labB,Variable(torch.zeros(realA.shape[0],1,30,30).to(cuda)))\n",
    "        error_discBA_with_realB=loss_mse(dis_real_labA,Variable(torch.ones(realA.shape[0],1,30,30).to(cuda)))\n",
    "        error_discBA_with_realB=loss_mse(dis_fake_labA,Variable(torch.zeros(realA.shape[0],1,30,30).to(cuda)))\n",
    "        \n",
    "        disc_loss=error_discAB_with_realB+error_discAB_with_fakeB+error_discBA_with_realB+error_discBA_with_realB\n",
    "        disc_loss.backward()\n",
    "        #error_discAB_with_fakeB.backward()\n",
    "        #error_discBA_with_realB.backward()\n",
    "        #error_discBA_with_realB.backward()\n",
    "        if count%10==0:\n",
    "            writer.add_image(\"realA\",vutils.make_grid(realA,padding=2,normalize=True),count)\n",
    "            writer.add_image(\"realB\",vutils.make_grid(realB,padding=2,normalize=True),count)\n",
    "            writer.add_image(\"fakeA\",vutils.make_grid(fakeA.view(fakeA.shape[0]),padding=2,normalize=True),count)\n",
    "            writer.add_image(\"fakeB\",vutils.make_grid(fakeB,padding=2,normalize=True),count)\n",
    "            writer.add_image(\"recA\",vutils.make_grid(recA,padding=2,normalize=True),count)\n",
    "            writer.add_image(\"recB\",vutils.make_grid(recB,padding=2,normalize=True),count)\n",
    "        \n",
    "        dis_optim.step()\n",
    "        \n",
    "        count=count+1\n",
    "        print(\"epoch:{}  disc_loss:{}  gen_loss:{} recA Loss:{} recB Loss:{}\".format(epoch,disc_loss,g_loss,rec_errorA\n",
    "                                                                                     ,rec_errorB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_fake_labB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(109.0262, device='cuda:2', grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_errorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 22 21:39:49 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P6            On   | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    77W /  90W |  14231MiB / 15261MiB |     96%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P6            On   | 00000000:45:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    92W /  90W |   9918MiB / 15261MiB |     99%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P6            On   | 00000000:C9:00.0 Off |                    0 |\n",
      "| N/A   53C    P0    71W /  90W |  12446MiB / 15261MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    127861      C   ...kaiser/anaconda3/envs/patra2/bin/python   763MiB |\n",
      "|    0    172293      C   ...kaiser/anaconda3/envs/patra2/bin/python   527MiB |\n",
      "|    0    210167      C   python                                     12923MiB |\n",
      "|    1      5493      C   ...kaiser/anaconda3/envs/dhriti/bin/python  8813MiB |\n",
      "|    1     22590      C   ...kaiser/anaconda3/envs/patra2/bin/python   473MiB |\n",
      "|    1     47203      C   ...kaiser/anaconda3/envs/dhriti/bin/python   523MiB |\n",
      "|    1    210167      C   python                                        91MiB |\n",
      "|    2    181326      C   ...kaiser/anaconda3/envs/patra2/bin/python 12337MiB |\n",
      "|    2    210167      C   python                                        91MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-55475d179a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'img'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
